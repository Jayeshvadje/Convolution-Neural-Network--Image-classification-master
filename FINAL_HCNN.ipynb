{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xlOsmc5GtS0X"
   },
   "outputs": [],
   "source": [
    "import keras as kr\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.datasets import cifar100\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from random import randint\n",
    "import time\n",
    "import os\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "joO_W4Q4oeo3"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')\n",
    "\n",
    "#if not os.path.exists('data/models/'):\n",
    "#  os.mkdir('data/models')\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aK7032XSe98l"
   },
   "outputs": [],
   "source": [
    "#if not os.path.exists('data/models/'):\n",
    "#  os.mkdir('data/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QMpsIa99tYfv"
   },
   "outputs": [],
   "source": [
    "# The number of coarse categories\n",
    "coarse_categories = 20\n",
    "\n",
    "# The number of fine categories\n",
    "fine_categories = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pipkQLJGtihz"
   },
   "outputs": [],
   "source": [
    "#Import Dataset\n",
    "(X, y_c), (x_test, y_test_c) = cifar100.load_data(label_mode='coarse')\n",
    "(X, y), (x_test, y_test) = cifar100.load_data(label_mode='fine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0DRcShN2tmLW"
   },
   "outputs": [],
   "source": [
    "#Fine-To-Coarse Mapping\n",
    "fine2coarse = np.zeros((fine_categories,coarse_categories))\n",
    "for i in range(coarse_categories):\n",
    "    index = np.where(y_test_c[:,0] == i)[0]\n",
    "    fine_cat = np.unique([y_test[j,0] for j in index])\n",
    "    for j in fine_cat:\n",
    "        fine2coarse[j,i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cV4BgPlH38jv"
   },
   "outputs": [],
   "source": [
    "# create antistoixo pinaka mono me auta pou theloume\n",
    "indecies = [84, 5, 20, 25, 94, 6, 7, 14, 18, 24] \n",
    "fine2coarseData = np.zeros((10,2))\n",
    "                         \n",
    "for i in range(10):\n",
    "  if fine2coarse[indecies[i],6] == 1.0:\n",
    "    fine2coarseData[i,0] = fine2coarse[indecies[i],6]\n",
    "  elif fine2coarse[indecies[i],7] == 1.0: \n",
    "    fine2coarseData[i,1] = fine2coarse[indecies[i],7]                  \n",
    "\n",
    "# The number of coarse categories\n",
    "coarse_categories = 2\n",
    "\n",
    "# The number of fine categories\n",
    "fine_categories = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d_tTgnsnQ3p0"
   },
   "outputs": [],
   "source": [
    "#fine2coarseData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "odQB4f5Ntojq"
   },
   "outputs": [],
   "source": [
    "#APO OLO TO SET KRATAME MONO AUTA POU MAS ENDIAFEROUN\n",
    "\n",
    "#SET gia ta fines\n",
    "# TODO only fine\n",
    "indices_train = np.where( (y == 84) | (y == 5) | (y == 20)|(y == 25)|(y == 94)|(y == 6)|(y == 7)|(y == 14)|(y == 18)|(y == 24))[0]\n",
    "indices_test = np.where((y_test == 84) | (y_test == 5) | (y_test == 20)|(y_test == 25)|(y_test == 94)|(y_test == 6)|(y_test == 7)|(y_test == 14)|(y_test == 18)|(y_test == 24))[0]\n",
    "\n",
    "# Reduce class labels\n",
    "y_train_f = np.array(y[indices_train])\n",
    "y_test_f = np.array(y_test[indices_test])\n",
    "# Reduce train- and test-data\n",
    "x_train = X[indices_train]\n",
    "x_test2 = x_test[indices_test]\n",
    "#giati pano me ta indices den to dexete\n",
    "x_test = x_test2\n",
    "  \n",
    "#SET gia ta coarses\n",
    "# Get indices to keep the right categories from dataset\n",
    "indices_train = np.where( (y_c == 6) | (y_c == 7))[0]\n",
    "indices_test = np.where((y_test_c == 6) | (y_test_c == 7))[0]\n",
    "\n",
    "# Reduce class labels\n",
    "y_train_c = np.array(y_c[indices_train])\n",
    "y_test_c = np.array(y_test_c[indices_test])\n",
    "#TA X EINAI IDIA EITE EINAI FINE EITE COARSE\n",
    "\n",
    "\n",
    "#GIA TO ONE HOT ENCODING\n",
    "y_train_f[np.array(y_train_f==84)]=0\n",
    "y_train_f[np.array(y_train_f==5)]=1\n",
    "y_train_f[np.array(y_train_f==20)]=2\n",
    "y_train_f[np.array(y_train_f==25)]=3  \n",
    "y_train_f[np.array(y_train_f==94)]=4\n",
    "y_train_f[np.array(y_train_f==6)]=5  \n",
    "y_train_f[np.array(y_train_f==7)]=6\n",
    "y_train_f[np.array(y_train_f==14)]=7  \n",
    "y_train_f[np.array(y_train_f==18)]=8\n",
    "y_train_f[np.array(y_train_f==24)]=9 \n",
    "y_test_f[np.array(y_test_f==84)]=0\n",
    "y_test_f[np.array(y_test_f==5)]=1 \n",
    "y_test_f[np.array(y_test_f==20)]=2\n",
    "y_test_f[np.array(y_test_f==25)]=3 \n",
    "y_test_f[np.array(y_test_f==94)]=4\n",
    "y_test_f[np.array(y_test_f==6)]=5 \n",
    "y_test_f[np.array(y_test_f==7)]=6\n",
    "y_test_f[np.array(y_test_f==14)]=7 \n",
    "y_test_f[np.array(y_test_f==18)]=8\n",
    "y_test_f[np.array(y_test_f==24)]=9 \n",
    "\n",
    "y_train_c[np.array(y_train_c==6)]=0\n",
    "y_train_c[np.array(y_train_c==7)]=1 \n",
    "y_test_c[np.array(y_test_c==6)]=0\n",
    "y_test_c[np.array(y_test_c==7)]=1\n",
    "\n",
    "#METATROPI SE ONE HOT\n",
    "y_train_c = to_categorical(y_train_c, 2)\n",
    "y_test_c = to_categorical(y_test_c, 2)\n",
    "y_train_f = to_categorical(y_train_f, 10)\n",
    "y_test_f = to_categorical(y_test_f, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ygsSVKK0twnG"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: ZCA\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This function applies ZCA Whitening to the image set\n",
    "#    \n",
    "#    Parameters:\n",
    "#        x_1           Array of MxNxC images to compute the ZCA Whitening\n",
    "#        x_2           Array of MxNxC images to apply the ZCA transform\n",
    "#        num_batch    Number of batches to do the computation\n",
    "# \n",
    "#    Returns:\n",
    "#        An array of MxNxC zca whitened images\n",
    "################################################################################\n",
    "def zca(x_1, x_2, epsilon=1e-5):\n",
    "        \n",
    "    with tf.name_scope('ZCA'):\n",
    "        \n",
    "        x1 = tf.placeholder(tf.float64, shape=np.shape(x_1), name='placeholder_x1')\n",
    "        x2 = tf.placeholder(tf.float64, shape=np.shape(x_2), name='placeholder_x2')\n",
    "        \n",
    "        flatx = tf.cast(tf.reshape(x1, (-1, np.prod(x_1.shape[-3:])),name=\"reshape_flat\"),tf.float64,name=\"flatx\")\n",
    "        sigma = tf.tensordot(tf.transpose(flatx),flatx, 1,name=\"sigma\") / tf.cast(tf.shape(flatx)[0],tf.float64) ### N-1 or N?\n",
    "        s, u, v = tf.svd(sigma,name=\"svd\")\n",
    "        pc = tf.tensordot(tf.tensordot(u,tf.diag(1. / tf.sqrt(s+epsilon)),1,name=\"inner_dot\"),tf.transpose(u),1, name=\"pc\")\n",
    "        \n",
    "        net1 = tf.tensordot(flatx, pc,1,name=\"whiten1\")\n",
    "        net1 = tf.reshape(net1,np.shape(x_1), name=\"output1\")\n",
    "        \n",
    "        flatx2 = tf.cast(tf.reshape(x2, (-1, np.prod(x_2.shape[-3:])),name=\"reshape_flat2\"),tf.float64,name=\"flatx2\")\n",
    "        net2 = tf.tensordot(flatx2, pc,1,name=\"whiten2\")\n",
    "        net2 = tf.reshape(net2,np.shape(x_2), name=\"output2\")\n",
    "        \n",
    "    with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            x_1,x_2 = sess.run([net1,net2], feed_dict={x1: x_1, x2: x_2})    \n",
    "    return x_1,x_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "aVp90CRCtzXB",
    "outputId": "e9406e5e-9257-49fa-dbff-bfa71ad2f072"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed - ZCA Whitening: 329.8146834373474\n"
     ]
    }
   ],
   "source": [
    "#treksimo tou ZCA\n",
    "time1 = time.time()\n",
    "x_train,x_test = zca(x_train,x_test)\n",
    "time2 = time.time()\n",
    "print('Time Elapsed - ZCA Whitening: '+str(time2-time1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z0ECfCU3vNK7"
   },
   "outputs": [],
   "source": [
    "#Split Training set into Training and Validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train_f, test_size=.2, random_state=2019, stratify = y_train_f )\n",
    "X = 0\n",
    "y = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3AhIzdJ1daH0"
   },
   "outputs": [],
   "source": [
    "#y_train_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JvOTxIkNvRFA"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: Preprocess Img\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This function pads images by 4 pixels, randomly crops them, then\n",
    "#        randomly flips them\n",
    "#    \n",
    "#    Parameters:\n",
    "#        x_1           Array of MxNxC images to compute the ZCA Whitening\n",
    "#        x_2           Array of MxNxC images to apply the ZCA transform\n",
    "#        num_batch    Number of batches to do the computation\n",
    "# \n",
    "#    Returns:\n",
    "#        An array of MxNxC zca whitened images\n",
    "################################################################################\n",
    "def preprocess_img(X,y):\n",
    "        \n",
    "    with tf.name_scope('Preproc'):\n",
    "        \n",
    "        images = tf.placeholder(tf.float64, shape=np.shape(X))\n",
    "        labels = tf.placeholder(tf.float64, shape=np.shape(y))\n",
    "        \n",
    "        net = tf.map_fn(lambda img: tf.image.flip_left_right(img), images)\n",
    "        net = tf.map_fn(lambda img: tf.image.rot90(img), net)\n",
    "        net = tf.image.resize_image_with_crop_or_pad(net,40,40)\n",
    "        net = tf.map_fn(lambda img: tf.random_crop(img, [32,32,3]), net)\n",
    "\n",
    "        net1 = tf.image.resize_image_with_crop_or_pad(images,40,40)\n",
    "        net1 = tf.map_fn(lambda img: tf.random_crop(img, [32,32,3]), net1)\n",
    "        \n",
    "        net = tf.concat([net, net1],0)\n",
    "        net = tf.random_shuffle(net, seed=0)\n",
    "        net_labels = tf.concat([labels, labels],0)\n",
    "        net_labels = tf.random_shuffle(net_labels,seed=0)\n",
    "        \n",
    "        net = tf.map_fn(lambda img: tf.image.random_flip_up_down(img), net)\n",
    "        \n",
    "    with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            x_t,y_t = sess.run([net,net_labels], feed_dict={images: X, labels: y})    \n",
    "    return x_t,y_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "3qTfj8ntvSsq",
    "outputId": "1e920f83-c3d5-4cce-d45c-3394b72fd81f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed - Img Preprocessing: 12.674590587615967\n"
     ]
    }
   ],
   "source": [
    "#treksimo tis proepeksergasias ton eikonon\n",
    "#Flip, pad and randomly crop each photo\n",
    "\n",
    "time1 = time.time()\n",
    "x_train,y_train = preprocess_img(x_train,y_train)\n",
    "time2 = time.time()\n",
    "print('Time Elapsed - Img Preprocessing: '+str(time2-time1));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RIEIv4aJv2wD"
   },
   "outputs": [],
   "source": [
    "#compile model\n",
    "model = Model(inputs=in_layer,outputs=net)\n",
    "sgd_coarse = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.5, nesterov=True)\n",
    "model.compile(optimizer= sgd_coarse, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bVh86DyPvyPu"
   },
   "outputs": [],
   "source": [
    "#Single Classifier Training\n",
    "# Constructing 1st CNN (shared layers)\n",
    "\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.layers import Input, Conv2D, Dropout, MaxPooling2D, Flatten, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "in_layer = Input(shape=(32, 32, 3), dtype='float32', name='main_input')\n",
    "\n",
    "net = Conv2D(32, 3, strides=1, padding='same', activation='elu')(in_layer)\n",
    "net = MaxPooling2D((2, 2), padding='valid')(net)\n",
    "\n",
    "net = Conv2D(64, 3, strides=1, padding='same', activation='elu')(net)\n",
    "net = Conv2D(64, 3, strides=1, padding='same', activation='elu')(net)\n",
    "net = Dropout(.2)(net)\n",
    "net = MaxPooling2D((2, 2), padding='valid')(net)\n",
    "\n",
    "net = Conv2D(64, 3, strides=1, padding='same', activation='elu')(net)\n",
    "net = Conv2D(64, 3, strides=1, padding='same', activation='elu')(net)\n",
    "net = Dropout(.3)(net)\n",
    "net = MaxPooling2D((2, 2), padding='valid')(net)\n",
    "\n",
    "\n",
    "net = Flatten()(net)\n",
    "net = Dense(512, activation='elu')(net)\n",
    "net = Dense(10, activation='softmax')(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tw1Nb_dNv7fA"
   },
   "outputs": [],
   "source": [
    "#tbCallBack = kr.callbacks.TensorBoard(log_dir='./data/graph/elu_drop/', histogram_freq=0, write_graph=True, write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SP6inFwddGMP"
   },
   "outputs": [],
   "source": [
    "#y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "colab_type": "code",
    "id": "Y6MYqotOwF2u",
    "outputId": "328d6dc8-948c-41b3-9d18-cc4e6876cc46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/2\n",
      "8000/8000 [==============================] - 3s 436us/step - loss: 2.2029 - acc: 0.1809 - val_loss: 2.0256 - val_acc: 0.2490\n",
      "Epoch 2/2\n",
      "8000/8000 [==============================] - 2s 268us/step - loss: 1.8819 - acc: 0.2996 - val_loss: 1.6900 - val_acc: 0.3650\n",
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 3/4\n",
      "8000/8000 [==============================] - 2s 267us/step - loss: 1.7049 - acc: 0.3687 - val_loss: 1.7070 - val_acc: 0.3770\n",
      "Epoch 4/4\n",
      "8000/8000 [==============================] - 2s 268us/step - loss: 1.5636 - acc: 0.4204 - val_loss: 1.5938 - val_acc: 0.4340\n",
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 5/6\n",
      "8000/8000 [==============================] - 2s 265us/step - loss: 1.4799 - acc: 0.4561 - val_loss: 1.4428 - val_acc: 0.4700\n",
      "Epoch 6/6\n",
      "8000/8000 [==============================] - 2s 266us/step - loss: 1.4061 - acc: 0.4834 - val_loss: 1.4571 - val_acc: 0.4900\n",
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 7/8\n",
      "8000/8000 [==============================] - 2s 268us/step - loss: 1.3494 - acc: 0.4986 - val_loss: 1.4032 - val_acc: 0.4910\n",
      "Epoch 8/8\n",
      "8000/8000 [==============================] - 2s 266us/step - loss: 1.3031 - acc: 0.5189 - val_loss: 1.4979 - val_acc: 0.4730\n",
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 2s 267us/step - loss: 1.2480 - acc: 0.5356 - val_loss: 1.3559 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 2s 264us/step - loss: 1.1968 - acc: 0.5589 - val_loss: 1.3420 - val_acc: 0.5030\n"
     ]
    }
   ],
   "source": [
    "#train model\n",
    "batch = 64\n",
    "index= 0\n",
    "step = 2\n",
    "stop = 10\n",
    "\n",
    "#vgike to callbacks=[tbCallBack]\n",
    "while index < stop:\n",
    "    model.fit(x_train, y_train, batch_size=batch, initial_epoch=index, epochs=index+step, validation_data=(x_val, y_val) )\n",
    "    index += step\n",
    "    model.save_weights('model_coarse'+str(index))\n",
    "save_index = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BiEk1QzlyeOJ"
   },
   "outputs": [],
   "source": [
    "#create fine optimizer\n",
    "sgd_fine = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BNmqEfXByj1V"
   },
   "outputs": [],
   "source": [
    "#epidi exoume shared layers, gia na min ta ksanakpaideoume\n",
    "for i in range(len(model.layers)):\n",
    "    model.layers[i].trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qpjBo-V1ylk5"
   },
   "outputs": [],
   "source": [
    "#einai apo tin train test split ta y_train kai y_val\n",
    "#ousiastika pairnei ta y coarses mesa apo ton pinaka geitniasis me esoteriko ginomeno\n",
    "y_train_c = np.dot(y_train,fine2coarseData)\n",
    "y_val_c = np.dot(y_val,fine2coarseData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SV3ORmGdhBTw"
   },
   "outputs": [],
   "source": [
    "#print ekteleso keli pou ipologizei to y_rain_c apo pano\n",
    "#print(y_train_c.shape)\n",
    "#print(y_train_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nqFJRc4bhPmW"
   },
   "outputs": [],
   "source": [
    "#print ekteleso keli pou ipologizei to y_rain_c apo pano\n",
    "#print(y_val.shape)\n",
    "#print(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s3Es2hJEiAVB"
   },
   "outputs": [],
   "source": [
    "#afou ekteleso to pano keli me to dot\n",
    "#y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gUKuIulshXq4"
   },
   "outputs": [],
   "source": [
    "#meta to treksimo me to dot\n",
    "#print(y_train_c.shape)\n",
    "#print(y_train_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "trdAZcjxypdP"
   },
   "outputs": [],
   "source": [
    "#Fine-Tuning for Coarse Classifier\n",
    "#orismos tou 2ou sineliktikou diktiou\n",
    "\n",
    "net = Conv2D(64, 3, strides=1, padding='same', activation='elu')(model.layers[-8].output)\n",
    "net = Conv2D(64, 3, strides=1, padding='same', activation='elu')(net)\n",
    "net = Dropout(.6)(net)\n",
    "net = MaxPooling2D((2, 2), padding='same')(net)\n",
    "\n",
    "net = Flatten()(net)\n",
    "net = Dense(512, activation='elu')(net)\n",
    "out_coarse = Dense(2, activation='softmax')(net)\n",
    "\n",
    "model_c = Model(inputs=in_layer,outputs=out_coarse)\n",
    "model_c.compile(optimizer= sgd_coarse, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "for i in range(len(model_c.layers)-1):\n",
    "    model_c.layers[i].set_weights(model.layers[i].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "colab_type": "code",
    "id": "qEPHhi2ayqdn",
    "outputId": "d86f373e-b60f-4c69-a812-4d8ff480715b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 31/40\n",
      "8000/8000 [==============================] - 2s 305us/step - loss: 0.2680 - acc: 0.8888 - val_loss: 0.2031 - val_acc: 0.9190\n",
      "Epoch 32/40\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 0.2190 - acc: 0.9136 - val_loss: 0.1965 - val_acc: 0.9230\n",
      "Epoch 33/40\n",
      "8000/8000 [==============================] - 1s 169us/step - loss: 0.2059 - acc: 0.9166 - val_loss: 0.1935 - val_acc: 0.9240\n",
      "Epoch 34/40\n",
      "8000/8000 [==============================] - 1s 169us/step - loss: 0.1947 - acc: 0.9237 - val_loss: 0.1900 - val_acc: 0.9220\n",
      "Epoch 35/40\n",
      "8000/8000 [==============================] - 1s 167us/step - loss: 0.1852 - acc: 0.9266 - val_loss: 0.1944 - val_acc: 0.9270\n",
      "Epoch 36/40\n",
      "8000/8000 [==============================] - 1s 169us/step - loss: 0.1793 - acc: 0.9283 - val_loss: 0.1938 - val_acc: 0.9250\n",
      "Epoch 37/40\n",
      "8000/8000 [==============================] - 1s 166us/step - loss: 0.1759 - acc: 0.9336 - val_loss: 0.1985 - val_acc: 0.9260\n",
      "Epoch 38/40\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 0.1674 - acc: 0.9364 - val_loss: 0.1995 - val_acc: 0.9250\n",
      "Epoch 39/40\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 0.1682 - acc: 0.9345 - val_loss: 0.1988 - val_acc: 0.9260\n",
      "Epoch 40/40\n",
      "8000/8000 [==============================] - 1s 170us/step - loss: 0.1632 - acc: 0.9371 - val_loss: 0.1938 - val_acc: 0.9230\n"
     ]
    }
   ],
   "source": [
    "#train tou deuterou sinelktikou\n",
    "index = 30\n",
    "step = 10\n",
    "stop = 40\n",
    "\n",
    "while index < stop:\n",
    "    model_c.fit(x_train, y_train_c, batch_size=batch, initial_epoch=index, epochs=index+step, validation_data=(x_val, y_val_c))\n",
    "    index += step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "colab_type": "code",
    "id": "tKo8qhOcyvcg",
    "outputId": "93433ccc-4e9b-489d-dd23-f51496858379"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 41/50\n",
      "8000/8000 [==============================] - 3s 315us/step - loss: 0.1508 - acc: 0.9417 - val_loss: 0.2008 - val_acc: 0.9240\n",
      "Epoch 42/50\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 0.1522 - acc: 0.9401 - val_loss: 0.2001 - val_acc: 0.9230\n",
      "Epoch 43/50\n",
      "8000/8000 [==============================] - 1s 166us/step - loss: 0.1517 - acc: 0.9387 - val_loss: 0.2015 - val_acc: 0.9290\n",
      "Epoch 44/50\n",
      "8000/8000 [==============================] - 1s 167us/step - loss: 0.1442 - acc: 0.9449 - val_loss: 0.2003 - val_acc: 0.9260\n",
      "Epoch 45/50\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 0.1461 - acc: 0.9407 - val_loss: 0.2069 - val_acc: 0.9260\n",
      "Epoch 46/50\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 0.1449 - acc: 0.9447 - val_loss: 0.2010 - val_acc: 0.9250\n",
      "Epoch 47/50\n",
      "8000/8000 [==============================] - 1s 169us/step - loss: 0.1408 - acc: 0.9433 - val_loss: 0.2067 - val_acc: 0.9240\n",
      "Epoch 48/50\n",
      "8000/8000 [==============================] - 1s 173us/step - loss: 0.1361 - acc: 0.9469 - val_loss: 0.2077 - val_acc: 0.9280\n",
      "Epoch 49/50\n",
      "8000/8000 [==============================] - 1s 167us/step - loss: 0.1407 - acc: 0.9419 - val_loss: 0.2164 - val_acc: 0.9210\n",
      "Epoch 50/50\n",
      "8000/8000 [==============================] - 1s 167us/step - loss: 0.1308 - acc: 0.9489 - val_loss: 0.2100 - val_acc: 0.9250\n"
     ]
    }
   ],
   "source": [
    "model_c.compile(optimizer=sgd_fine, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "stop = 50\n",
    "\n",
    "while index < stop:\n",
    "    model_c.fit(x_train, y_train_c, batch_size=batch, initial_epoch=index, epochs=index+step, validation_data=(x_val, y_val_c))\n",
    "    index += step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M0gT44eTyxuj"
   },
   "outputs": [],
   "source": [
    "#Fine-Tuning for Fine Classifiers\n",
    "#dimourgoume tous 2 fine classifiers gia ta dedomena mas\n",
    "def fine_model():\n",
    "    net = Conv2D(64, 3, strides=1, padding='same', activation='elu')(model.layers[-8].output)\n",
    "    net = Conv2D(64, 3, strides=1, padding='same', activation='elu')(net)\n",
    "    net = Dropout(.6)(net)\n",
    "    net = MaxPooling2D((2, 2), padding='same')(net)\n",
    "\n",
    "    net = Flatten()(net)\n",
    "    net = Dense(512, activation='elu')(net)\n",
    "    out_fine = Dense(10, activation='softmax')(net)\n",
    "    model_fine = Model(inputs=in_layer,outputs=out_fine)\n",
    "    model_fine.compile(optimizer= sgd_coarse,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    for i in range(len(model_fine.layers)-1):\n",
    "        model_fine.layers[i].set_weights(model.layers[i].get_weights())\n",
    "    return model_fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NXS9E0VNyylE"
   },
   "outputs": [],
   "source": [
    "fine_models = {'models' : [{} for i in range(coarse_categories)], 'yhf' : [{} for i in range(coarse_categories)]}\n",
    "for i in range(coarse_categories):\n",
    "    model_i = fine_model()\n",
    "    fine_models['models'][i] = model_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LNAgokVhy0Q-"
   },
   "outputs": [],
   "source": [
    "def get_error(y,yh):\n",
    "#    # Threshold \n",
    "    yht = np.zeros(np.shape(yh))\n",
    "    yht[np.arange(len(yh)), yh.argmax(1)] = 1\n",
    "    # Evaluate Error\n",
    "    error = np.count_nonzero(np.count_nonzero(y-yht,1))/len(y)\n",
    "    return error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 877
    },
    "colab_type": "code",
    "id": "9btIDnr_y2BB",
    "outputId": "86189696-70cd-4673-9bac-a50852aa0669"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 500 samples\n",
      "Epoch 1/5\n",
      "4000/4000 [==============================] - 2s 518us/step - loss: 1.4422 - acc: 0.4263 - val_loss: 1.2897 - val_acc: 0.5080\n",
      "Epoch 2/5\n",
      "4000/4000 [==============================] - 1s 189us/step - loss: 1.1939 - acc: 0.5162 - val_loss: 1.1637 - val_acc: 0.5560\n",
      "Epoch 3/5\n",
      "4000/4000 [==============================] - 1s 190us/step - loss: 1.1298 - acc: 0.5420 - val_loss: 1.1576 - val_acc: 0.5760\n",
      "Epoch 4/5\n",
      "4000/4000 [==============================] - 1s 189us/step - loss: 1.0738 - acc: 0.5745 - val_loss: 1.1681 - val_acc: 0.5460\n",
      "Epoch 5/5\n",
      "4000/4000 [==============================] - 1s 189us/step - loss: 1.0445 - acc: 0.5857 - val_loss: 1.1229 - val_acc: 0.5780\n",
      "Train on 4000 samples, validate on 500 samples\n",
      "Epoch 6/10\n",
      "4000/4000 [==============================] - 2s 496us/step - loss: 0.9896 - acc: 0.6135 - val_loss: 1.1074 - val_acc: 0.5840\n",
      "Epoch 7/10\n",
      "4000/4000 [==============================] - 1s 184us/step - loss: 0.9794 - acc: 0.6102 - val_loss: 1.1137 - val_acc: 0.5760\n",
      "Epoch 8/10\n",
      "4000/4000 [==============================] - 1s 186us/step - loss: 0.9621 - acc: 0.6298 - val_loss: 1.1438 - val_acc: 0.5700\n",
      "Epoch 9/10\n",
      "4000/4000 [==============================] - 1s 189us/step - loss: 0.9426 - acc: 0.6345 - val_loss: 1.1040 - val_acc: 0.5840\n",
      "Epoch 10/10\n",
      "4000/4000 [==============================] - 1s 190us/step - loss: 0.9288 - acc: 0.6300 - val_loss: 1.1326 - val_acc: 0.5760\n",
      "Fine Classifier 0 Error: 0.424\n",
      "Train on 4000 samples, validate on 500 samples\n",
      "Epoch 1/5\n",
      "4000/4000 [==============================] - 2s 541us/step - loss: 1.2979 - acc: 0.4913 - val_loss: 1.1887 - val_acc: 0.5140\n",
      "Epoch 2/5\n",
      "4000/4000 [==============================] - 1s 186us/step - loss: 1.0515 - acc: 0.5837 - val_loss: 1.1697 - val_acc: 0.5440\n",
      "Epoch 3/5\n",
      "4000/4000 [==============================] - 1s 187us/step - loss: 0.9998 - acc: 0.6035 - val_loss: 1.1064 - val_acc: 0.5640\n",
      "Epoch 4/5\n",
      "4000/4000 [==============================] - 1s 190us/step - loss: 0.9438 - acc: 0.6292 - val_loss: 1.0983 - val_acc: 0.5740\n",
      "Epoch 5/5\n",
      "4000/4000 [==============================] - 1s 190us/step - loss: 0.9040 - acc: 0.6430 - val_loss: 1.1405 - val_acc: 0.5880\n",
      "Train on 4000 samples, validate on 500 samples\n",
      "Epoch 6/10\n",
      "4000/4000 [==============================] - 2s 554us/step - loss: 0.8554 - acc: 0.6633 - val_loss: 1.0659 - val_acc: 0.5880\n",
      "Epoch 7/10\n",
      "4000/4000 [==============================] - 1s 187us/step - loss: 0.8293 - acc: 0.6767 - val_loss: 1.0525 - val_acc: 0.5800\n",
      "Epoch 8/10\n",
      "4000/4000 [==============================] - 1s 188us/step - loss: 0.8148 - acc: 0.6877 - val_loss: 1.0379 - val_acc: 0.5920\n",
      "Epoch 9/10\n",
      "4000/4000 [==============================] - 1s 188us/step - loss: 0.8109 - acc: 0.6885 - val_loss: 1.0441 - val_acc: 0.5820\n",
      "Epoch 10/10\n",
      "4000/4000 [==============================] - 1s 187us/step - loss: 0.8003 - acc: 0.6920 - val_loss: 1.0447 - val_acc: 0.5940\n",
      "Fine Classifier 1 Error: 0.406\n"
     ]
    }
   ],
   "source": [
    "#Train Fine Classifiers on Respective Data\n",
    "for i in range(coarse_categories):\n",
    "    index= 0\n",
    "    step = 5\n",
    "    stop = 5\n",
    "    \n",
    "    # Get all training data for the coarse category\n",
    "    ix = np.where([(y_train[:,j]==1) for j in [k for k, e in enumerate(fine2coarseData[:,i]) if e != 0]])[1]\n",
    "    x_tix = x_train[ix]\n",
    "    y_tix = y_train[ix]\n",
    "    \n",
    "    # Get all validation data for the coarse category\n",
    "    ix_v = np.where([(y_val[:,j]==1) for j in [k for k, e in enumerate(fine2coarseData[:,i]) if e != 0]])[1]\n",
    "    x_vix = x_val[ix_v]\n",
    "    y_vix = y_val[ix_v]\n",
    "    \n",
    "    while index < stop:\n",
    "        fine_models['models'][i].fit(x_tix, y_tix, batch_size=batch, initial_epoch=index, epochs=index+step, validation_data=(x_vix, y_vix))\n",
    "        index += step\n",
    "    \n",
    "    fine_models['models'][i].compile(optimizer=sgd_fine, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    stop = 10\n",
    "\n",
    "    while index < stop:\n",
    "        fine_models['models'][i].fit(x_tix, y_tix, batch_size=batch, initial_epoch=index, epochs=index+step, validation_data=(x_vix, y_vix))\n",
    "        index += step\n",
    "        \n",
    "    yh_f = fine_models['models'][i].predict(x_val[ix_v], batch_size=batch)\n",
    "    print('Fine Classifier '+str(i)+' Error: '+str(get_error(y_val[ix_v],yh_f)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "colab_type": "code",
    "id": "x-qaPz5Smy3V",
    "outputId": "f99216c6-bc3a-4e70-b3ea-c440efc32b49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 581us/step\n",
      "Model coarse\n",
      "0.9\n",
      "Pososta ana klasi\n",
      "['household furniture = 0.834', 'insects = 0.966']\n"
     ]
    }
   ],
   "source": [
    "#ipologismos accuracy gia ta cnn mas\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "#ipologismos Acuuracy gia coarse genikos\n",
    "y_pred = model_c.predict(x_test,batch_size=64,verbose=1)\n",
    "Y_pred = np.argmax(y_pred,axis=1)\n",
    "Y_test = np.argmax(y_test_c,axis=1)\n",
    "accuracy = (len(Y_test) - np.count_nonzero(Y_pred - Y_test))/len(Y_test)\n",
    "print('Model coarse')\n",
    "print(accuracy)\n",
    "\n",
    "#accuracy gia kathe klassi xorista\n",
    "cifar_classes = [\"household furniture\",\"insects\"]\n",
    "\n",
    "cm =confusion_matrix(Y_test,Y_pred)\n",
    "#print(cm)\n",
    "acc_per_label = cm.diagonal()/cm.sum(axis=1)\n",
    "acc_per_label_final = [cifar_classes[i]+ \" = \"+str(acc_per_label[i]) for i in range(0,2)]\n",
    "print('Pososta ana klasi')\n",
    "print(acc_per_label_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "id": "uiKUFicKFN9g",
    "outputId": "ea1fd72e-c7a9-4e80-b6d9-0dd3f543cbf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 93us/step\n",
      "Evaluating Fine Classifier:  0\n",
      "Model fine: household furniture\n",
      "0.396\n",
      "accuracies for each class seperately\n",
      "['table = 0.17', 'bed = 0.19', 'chair = 0.22', 'couch = 0.83', 'wardrobe = 0.57']\n"
     ]
    }
   ],
   "source": [
    "#ipologismos Accuracy gia ton fine classifier gia ta epipla\n",
    "#arxika kanoume probabilistic averaging me ta coarses\n",
    "\n",
    "#apo ta y_test_f kratao mono auta gia ta epipla\n",
    "Y_test = np.argmax(y_test_f,axis=1)\n",
    "indices_test = np.where((Y_test == 0) | (Y_test == 1) | (Y_test == 2)|(Y_test == 3)|(Y_test == 4))[0]\n",
    "x_test3 = x_test[indices_test]\n",
    "y_test_f3 =  np.array(y_test_f[indices_test])\n",
    "\n",
    "#probabilistic averaging\n",
    "#iplogismos \"varon\" apo coarses\n",
    "yh = np.zeros(np.shape(y_test_f3))\n",
    "yh_c = model_c.predict(x_test3,batch_size=64,verbose=1)\n",
    "print(\"Evaluating Fine Classifier: \", str(0))\n",
    "fine_models['yhf'][0] = fine_models['models'][0].predict(x_test3, batch_size=64)\n",
    "yh = np.multiply(yh_c[:,0].reshape((len(y_test_f3)),1), fine_models['yhf'][0])\n",
    "\n",
    "#ara to yh einai to kainourgio mas y_pred, opote\n",
    "Y_pred = np.argmax(yh,axis=1)\n",
    "Y_test = np.argmax(y_test_f3,axis=1)\n",
    "accuracy = (len(Y_test) - np.count_nonzero(Y_pred - Y_test))/len(Y_test)\n",
    "#score = model.evaluate(x_test, y_test, batch_size=64, verbose=1)\n",
    "print('Model fine: household furniture')\n",
    "print(accuracy)\n",
    "#print(score)\n",
    "\n",
    "#ipologismos gia kathe classi xorista\n",
    "cifar_classes = [\"table\", \"bed\",\"chair\", \"couch\",\"wardrobe\"]\n",
    "\n",
    "cm =confusion_matrix(Y_test,Y_pred)\n",
    "#print(cm)\n",
    "acc_per_label = cm.diagonal()/cm.sum(axis=1)\n",
    "acc_per_label_final = [cifar_classes[i]+ \" = \"+str(acc_per_label[i]) for i in range(0,5)]\n",
    "print('accuracies for each class seperately')\n",
    "print(acc_per_label_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "id": "UU3Velk-FOZW",
    "outputId": "d1834475-4f44-4944-cd21-ebc0f7f0c368"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 94us/step\n",
      "Evaluating Fine Classifier:  0\n",
      "Model fine: insects\n",
      "0.568\n",
      "accuracies for each class seperately\n",
      "['bee = 0.22', 'beetle = 0.57', 'butterfly = 0.59', 'caterpillar = 0.72', 'cockroach = 0.74']\n"
     ]
    }
   ],
   "source": [
    "#ipologismos Accuracy gia ton fine classifier gia ta entoma\n",
    "#gia tous fine classifiers ginetai Probabilistic Averaging me vasi ta coarses\n",
    "\n",
    "\n",
    "#apo ta y_test_f kratao mono auta gia ta epipla\n",
    "Y_test = np.argmax(y_test_f,axis=1)\n",
    "indices_test = np.where((Y_test == 5) | (Y_test == 6) | (Y_test == 7)|(Y_test == 8)|(Y_test == 9))[0]\n",
    "x_test4 = x_test[indices_test]\n",
    "y_test_f4 =  np.array(y_test_f[indices_test])\n",
    "\n",
    "#probabilistic averaging\n",
    "#iplogismos \"varon\" apo coarses\n",
    "yh = np.zeros(np.shape(y_test_f4))\n",
    "yh_c = model_c.predict(x_test4,batch_size=64,verbose=1)\n",
    "print(\"Evaluating Fine Classifier: \", str(0))\n",
    "fine_models['yhf'][1] = fine_models['models'][1].predict(x_test4, batch_size=64)\n",
    "yh = np.multiply(yh_c[:,0].reshape((len(y_test_f4)),1), fine_models['yhf'][1])\n",
    "\n",
    "#ara to yh einai to kainourgio mas y_pred, opote\n",
    "Y_pred = np.argmax(yh,axis=1)\n",
    "Y_test = np.argmax(y_test_f4,axis=1)\n",
    "accuracy = (len(Y_test) - np.count_nonzero(Y_pred - Y_test))/len(Y_test)\n",
    "#score = model.evaluate(x_test, y_test, batch_size=64, verbose=1)\n",
    "print('Model fine: insects')\n",
    "print(accuracy)\n",
    "#print(score)\n",
    "\n",
    "cifar_classes = [\"bee\",\"beetle\", \"butterfly\",\"caterpillar\", \"cockroach\"]\n",
    "\n",
    "cm =confusion_matrix(Y_test,Y_pred)\n",
    "#print(cm)\n",
    "acc_per_label = cm.diagonal()/cm.sum(axis=1)\n",
    "acc_per_label_final = [cifar_classes[i]+ \" = \"+str(acc_per_label[i]) for i in range(0,5)]\n",
    "print('accuracies for each class seperately')\n",
    "print(acc_per_label_final)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FINAL HCNN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
